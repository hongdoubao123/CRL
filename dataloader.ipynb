{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c5fb94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T11:25:01.814204Z",
     "start_time": "2024-09-04T11:24:50.423022Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "# Resolved an issue where images are too large to be processed by PIL\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import  torchvision\n",
    "from torchvision import datasets, transforms\n",
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\")\n",
    "device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ef9f5cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T11:26:02.025853Z",
     "start_time": "2024-09-04T11:26:02.002494Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_split_data(root: str, tra_rate: float = 0.8, val_rate: float = 0.1, test_rate: float = 0.1):\n",
    "    random.seed(42)  \n",
    "    assert os.path.exists(\n",
    "        root), \"dataset root: {} does not exist.\".format(root)\n",
    "\n",
    "    object_class = [cla for cla in sorted(os.listdir(root)) if os.path.isdir(os.path.join(root, cla))]\n",
    "    num_classes = len(object_class)\n",
    "\n",
    "    class_indices = dict((k, v) for v, k in enumerate(object_class))\n",
    "    json_str = json.dumps(dict((val, key) for key, val in class_indices.items()), indent=4)\n",
    "    with open('class_indices.json', 'w') as json_file:\n",
    "        json_file.write(json_str)\n",
    "\n",
    "    train_images_path = []  \n",
    "    train_images_label = [] \n",
    "    val_images_path = []  \n",
    "    val_images_label = [] \n",
    "    test_images_path = []  \n",
    "    test_images_label = []  \n",
    "\n",
    "    every_class_num = []  \n",
    "    supported = [\".jpg\", \".JPG\", \".png\", \".PNG\"]  \n",
    "   \n",
    "    for cla in object_class:\n",
    "        cla_path = os.path.join(root, cla)\n",
    "        images = [os.path.join(root, cla, i) for i in sorted(os.listdir(cla_path))\n",
    "                  if os.path.splitext(i)[-1] in supported]\n",
    "        image_class = class_indices[cla]\n",
    "        total = len(images)\n",
    "        every_class_num.append(total)\n",
    "        random.shuffle(images)\n",
    "        train_images = images[0:int(tra_rate*total)]  \n",
    "        val_images = images[int(tra_rate*total):int((tra_rate+val_rate)*total)] \n",
    "        test_images = images[int((tra_rate+val_rate)*total):]\n",
    "\n",
    "        for img_path in images:\n",
    "            if img_path in train_images:\n",
    "                train_images_path.append(img_path)\n",
    "                train_images_label.append(image_class)\n",
    "            if img_path in val_images:\n",
    "                val_images_path.append(img_path)\n",
    "                val_images_label.append(image_class)\n",
    "            if img_path in test_images:\n",
    "                test_images_path.append(img_path)\n",
    "                test_images_label.append(image_class)\n",
    "        print(cla, ':', 'count:', total, ' train:', len(train_images),\n",
    "              'val:', len(val_images), 'test:', len(test_images))\n",
    "\n",
    "    print(\n",
    "        f\"There are {len(object_class)} class and {sum(every_class_num)} images were found in the dataset.\")\n",
    "    print(\"{} images for train.\".format(len(train_images_path)))\n",
    "    print(\"{} images for validation.\".format(len(val_images_path)))\n",
    "    print(\"{} images for test.\".format(len(test_images_path)))\n",
    "\n",
    "    plot_image = True\n",
    "    save_image = True\n",
    "   \n",
    "    return train_images_path, val_images_path, test_images_path ,num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e61aee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T11:28:18.219227Z",
     "start_time": "2024-09-04T11:28:17.883341Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path1 = r'./Datasets/AiRound/aerial/'\n",
    "path2 = r'./Datasets/New_AiRound/aerial/'\n",
    "train_images_path1, train_images_label1,val_images_path, val_images_label,test_images_path, test_images_label=read_split_data(path1,0.7,0.1,0.2)\n",
    "train_images_path2, train_images_label2,_,_,_,_=read_split_data(path2,1,0,0)\n",
    "dele_path = val_images_path+test_images_path\n",
    "dele_label = val_images_label+test_images_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59539c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T11:28:20.778350Z",
     "start_time": "2024-09-04T11:28:20.735137Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_path(dele_path , train_images_path):\n",
    "    dele = []\n",
    "    for i in range(0 , len(dele_path)):\n",
    "        dele.append(dele_path[i].split(\"/\")[-2]+\"/\"+ dele_path[i].split(\"/\")[-1].split('.')[0])\n",
    "    whole = []\n",
    "    for k in range(0 , len(train_images_path2)):\n",
    "        whole.append(train_images_path[k].split('/')[-2]+\"/\"+train_images_path[k].split('/')[-1])\n",
    "    \n",
    "    conindex = []\n",
    "    unconindex = []\n",
    "    con =[]\n",
    "    uncon = []\n",
    "    for t in range(0,len(whole)):\n",
    "        if whole[t].split('.')[0] in dele:\n",
    "            con.append(t)\n",
    "            conindex.append(whole[t])\n",
    "        else:\n",
    "            uncon.append(t)\n",
    "            unconindex.append(whole[t])\n",
    "    return con , conindex , uncon , unconindex\n",
    "\n",
    "def trans(label,con,uncon):\n",
    "    scon =[]\n",
    "    suncon = []\n",
    "    for i in con:\n",
    "        scon.append(label[i])\n",
    "    for j in uncon:\n",
    "        suncon.append(label[j])\n",
    "    return scon,suncon\n",
    "\n",
    "\n",
    "\n",
    "con , conindex , uncon , unconindex = select_path(dele_path , train_images_path2) \n",
    "new_con,new_uncon = trans(train_images_path2,con,uncon)\n",
    "new_conlabel,new_unconlabel = trans(train_images_label2,con,uncon)\n",
    "#==========================================================#\n",
    "train_dataset_path = new_uncon\n",
    "train_dataset_label = new_unconlabel\n",
    "clean_val_dataset_path = val_images_path\n",
    "clean_val_dataset_label = val_images_label\n",
    "clean_test_dataset_path = test_images_path\n",
    "clean_test_dataset_label = test_images_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d55dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "#     transforms.RandomResizedCrop(224,scale=(0.7,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.6959, 0.6537, 0.6371), (0.3113, 0.3192, 0.3214)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    \n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "#     transforms.RandomResizedCrop(224,scale=(0.7,1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.6959, 0.6537, 0.6371), (0.3113, 0.3192, 0.3214)),\n",
    "\n",
    "])\n",
    "class Add(Dataset):\n",
    "    def __init__(self, Dataset,Labels,transform = None,target_transform = None):\n",
    "        self.List = Dataset\n",
    "        self.Labels = Labels\n",
    "        self.length = len(self.List)\n",
    "        if transform is None:\n",
    "            self.transform = transforms.ToTensor()\n",
    "        else:\n",
    "            self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.List[index]).convert(\"RGB\")\n",
    "        label = self.Labels[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            label = self.target_transform(label)\n",
    "        return img,label,index\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def getData(self):\n",
    "        return self.List, self.Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = Add(train_dataset_path,train_dataset_label,transform = transform_train)\n",
    "val_images = Add(clean_val_dataset_path,clean_val_dataset_label,transform = transform_train)\n",
    "test_images = Add(clean_test_dataset_path,clean_test_dataset_label,transform = transform_test)\n",
    "train_loader = DataLoader(dataset=train_images, batch_size=16, num_workers=8, pin_memory=True, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(dataset=val_images, batch_size=8, num_workers=8, pin_memory=True, shuffle=False, drop_last=False)\n",
    "test_loader = DataLoader(dataset=test_images, batch_size=8, num_workers=8, pin_memory=True, shuffle=False, drop_last=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
